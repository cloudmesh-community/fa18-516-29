
Hadoop,Hive and Spark cluster set up on Amazon EC2 instances(Part-2):

The following section describes the installation of Hadoop on the 4 instances and the configurations required for the different config files:

Cluster Planning: We will have a 3 node Hadoop cluster with 1 as master and 3 as slaves.Each instance is T2.meduim type and has 4 GB of memory
and initially 8GB of physical storage.We will add more volume and upgrade the physical storage to 16GB.This configuration is fine for doing 
the initial cluster set up and running some map reduce jobs for test purpose,but definitely will not work for large datasets.For a 
substantial amount of data transfer,one should go for T2.Xlarge instances which have 16GB of memory and initially 32 GB of physical storage.
The only downside is that it will be expensive to own 4 such large instances.

1.Hadoop installation:(Everything is done when logged in as ubuntu user)

 i. Before starting the installation,upgrade all the servers as a good practice by the below command.
      sudo apt-get update
   
 ii. Install java version8 in all the servers:
      sudo apt install openjdk-8-jdk
    
 iii. Download and install hadoop 2.9 is all the servers.
      wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.9.1/hadoop-2.9.1.tar.gz -P ~/hadoop_installation
    
 iv. Uncompress the tar file in any directory called hadoop home
      tar zxvf ~/hadoop_installation/hadoop-* -C ~/hadoop_home
 
 v. set up the env variables in all the .profile and .bashrc of all the servers
 
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    export PATH=$PATH:$JAVA_HOME/bin
    export HADOOP_HOME=/home/ubuntu/hadoop_home/hadoop-2.9.1
    export PATH=$PATH:$HADOOP_HOME/bin
    export HADOOP_CONF_DIR=/home/ubuntu/hadoop_home/hadoop-2.9.1/etc/hadoop
   
  vi. Load profile in all the servers
     ~/.profile
     
  vii. Change the hadoop-env.sh in $HADOOP_HOME/etc/hadoop in all the instances to add the below line for JAVA_HOME.
     export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
     
  2. Hadoop configurations(The config files will be changed on all the instances):
  
    i. core-site.xml
    
      fs.defaultFS     hdfs://ec2-52-24-204-101.us-west-2.compute.amazonaws.com:9001(This is the master instance hostname,
                       9001 is the RPC port)
                       
      hadoop.tmp.dir  file:///home/ubuntu/hadoop_home/hadoop-2.9.1/hadoop_tmp(The filesystem should be the one where we are adding more
                       volume as there is lot of intermediate data which gets written here.
                       
.   ii. hdfs-site.xml

        dfs.namenode.name.dir  file:///home/ubuntu/hadoop_home/hadoop-2.9.1/hadoop_data/hdfs/namenode
                             (This is the namenode directory in the master instance which will have the fs image and edit logs)
                             
        dfs.datanode.data.dir  file:///home/ubuntu/hadoop_home/hadoop-2.9.1/hadoop_data/hdfs/datanode
                             (This is the directory in the slave nodes with the actual data blocks of distributed file system)
                             
        dfs.replication       2 (Initially only 2 considering the small size of physical storage)

        
