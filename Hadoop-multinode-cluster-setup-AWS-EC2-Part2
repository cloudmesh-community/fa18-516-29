
Hadoop,Hive and Spark cluster set up on Amazon EC2 instances(Part-2):

The following section describes the installation of Hadoop on the 4 instances and the configurations required for the different config files:

Cluster Planning: We will have a 3 node Hadoop cluster with 1 as master and 3 as slaves.Each instance is T2.meduim type and has 4 GB of memory
and initially 8GB of physical storage.We will add more volume and upgrade the physical storage to 16GB.This configuration is fine for doing 
the initial cluster set up and running some map reduce jobs for test purpose,but definitely will not work for large datasets.For a 
substantial amount of data transfer,one should go for T2.Xlarge instances which have 16GB of memory and initially 32 GB of physical storage.
The only downside is that it will be expensive to own 4 such large instances.

Master and Slaves:There will be 1 master instance.This will have the namemode,resourcemanager and jobhistoryserver deamons running.
There will be 3 slave nodes where datanode and nodemanager deamons will run.To configure masters and slaves create 2 files with names
masters and slaves inside $HADOOP_CONF_DIR in each instance.Add the following to them:

masters file                               slaves file
namenode                                    datanode1
                                            datanode2
                                            datanode3

Hadoop installation:(Everything is done when logged in as ubuntu user)

 i. Before starting the installation,upgrade all the servers as a good practice by the below command.
      sudo apt-get update
   
 ii. Install java version8 in all the servers:
      sudo apt install openjdk-8-jdk
    
 iii. Download and install hadoop 2.9 is all the servers.
      wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.9.1/hadoop-2.9.1.tar.gz -P ~/hadoop_installation
    
 iv. Uncompress the tar file in any directory called hadoop home
      tar zxvf ~/hadoop_installation/hadoop-* -C ~/hadoop_home
 
 v. set up the env variables in all the .profile and .bashrc of all the servers
 
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    export PATH=$PATH:$JAVA_HOME/bin
    export HADOOP_HOME=/home/ubuntu/hadoop_home/hadoop-2.9.1
    export PATH=$PATH:$HADOOP_HOME/bin
    export HADOOP_CONF_DIR=/home/ubuntu/hadoop_home/hadoop-2.9.1/etc/hadoop
   
  vi. Load profile in all the servers
     ~/.profile
     
  vii. Change the hadoop-env.sh in $HADOOP_HOME/etc/hadoop in all the instances to add the below line for JAVA_HOME.
     export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
     
  2. Hadoop configurations in $HADOOP_CONF_DIR:(Add the below values in the config files in all the instances):
  
    i. core-site.xml
    
        fs.defaultFS     hdfs://ec2-52-24-204-101.us-west-2.compute.amazonaws.com:9001(This is the master instance hostname,
                       9001 is the RPC port)
                       
        hadoop.tmp.dir  file:///home/ubuntu/hadoop_home/hadoop-2.9.1/hadoop_tmp(The filesystem should be the one where we are adding more
                       volume as there is lot of intermediate data which gets written here.
                       
.   ii. hdfs-site.xml

          dfs.namenode.name.dir  file:///home/ubuntu/hadoop_home/hadoop-2.9.1/hadoop_data/hdfs/namenode
                             (This is the namenode directory in the master instance which will have the fs image and edit logs)
                             
          dfs.datanode.data.dir  file:///home/ubuntu/hadoop_home/hadoop-2.9.1/hadoop_data/hdfs/datanode
                             (This is the directory in the slave nodes with the actual data blocks of distributed file system)
                             
          dfs.replication       2 (Initially only 2 considering the small size of physical storage)
        
     iii. mapred-site.xml
     
            mapreduce.framework.name   yarn
            yarn.app.mapreduce.am.resource.mb 3072
            mapreduce.map.memory.mb  512
            mapreduce.reduce.memory.mb 512
          
     iv. yarn-site.xml
       
           yarn.acl.enable  false
           yarn.resourcemanager.hostname ec2-52-24-204-101.us-west-2.compute.amazonaws.com
           yarn.nodemanager.aux-services mapreduce_shuffle
           yarn.nodemanager.vmem-check-enabled false
           yarn.nodemanager.resource.memory-mb 3072
           yarn.scheduler.maximum-allocation-mb 3072
           yarn.scheduler.minimum-allocation-mb 1536
           yarn.nodemanager.local-dirs ${hadoop.tmp.dir}/nm-local-dir
           
    3. Hadoop namenode format: After configuring the config files,format the namenode:
    
           hdfs namenode -format
           
    4. Hadoop deamons:
    
        Start the following deamons:
        
        On Namenode:
         $HADOOP_HOME/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
         $HADOOP_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager
         $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver
         
        On DataNode1:
         $HADOOP_HOME/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs start datanode
         The datanode process just needs to be started in 1 node.This will start the process in another nodes on its own through
         ssh connection.
         
        On DataNode1,DataNode2,DataNode3:
         The nodemanager process needs to be started in all the slave machines.
         $HADOOP_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start nodemanager
         
      5. Creating directories in file system:
        
         hadoop fs -mkdir /user/externaltables/testdata
         hadoop fs -copyFromLocal /home/ubuntu/datafiles/testfile /user/externaltables/testdata/testfile
         
      6. Run the map reduce job to test that the containers are getting launched properly in all the nodes:
         
         yarn jar hadoop-mapreduce-examples-2.9.1.jar wordcount /user/externaltables/testdata/testfile /user/logs
          
    Issues and resolution: One very common issue can come when the containers are launced on the nodes is that of 
    lack of space in the temp stoarge for the intermediate data which is generated.The error looks like:
     
     [INFO] Diagnostics: No space available in any of the local directories.
     
     To resolve this issue,find the filesystem to which hadoop.temp.dir is mapped to:
       
       df /tmp/hadoop
       
       df -h will show the space available in the above file system.If it is nearly full,then take more volume for the EC2
       instances:
       
         
           
          

        
