
Hadoop,Hive and Spark cluster set up on Amazon EC2 instances. 

The following section describes how to set up a multi node Hadoop cluster in Amazon EC2 instance.It also describes how to install and set up Spark
and Hive as part of this cluster set up.

1. Configuring EC2 instances in AWS and Setting up passwordless ssh communication bewteen these instances and configuring the Elastic IPs 
   in EC2 instances.

   Motivation: Before setting up a Hadoop cluster,it is important to set up connection between our client machine and the EC2 instances as 
   well as passwordless communication between the EC2 instances.Also,the IP of EC2 instances changes when we restart the servers.Since,we
   will need to stop and restart the machines only when we are using them,it is important to set up elastic ips for these instances,which
   do not change else our passwordless ssh connection set up will not work.
   
   Memory and Disk Configurations required: In AWS,create an EC2 instance of type T2.Medium by following the guidelines given in EC2 manual 
   of how to create an instance.The T2 Medium instance has 4GB of memory and initially 8GB of physical storage.In order to launch containers 
   in Hadoop and Spark on a yarn cluster,we found that 4GB of memory in the nodes is the mimimum required.If the memory is below this limit,
   the Application Master on the slave nodes will not start and the jobs will always be in ACCEPTED state.They will never get allocated to 
   Application master in slave nodes by the Resource Manager running in master node due to lack of resources.In this scenario also we have 
   to specify the memory configurations in yarn-site.xml and mapreduce-site.xml for each process as Hadoop defaults do not work properly
   if the memory is less than 8GB.The physical storage of EC2 instance is 8GB and everything is mapped to the drive /dev/xvda.There are few 
   challenges here as there are a lot of log files and local intermediate data which gets written in the datanodes when a mapreduce or spark 
   job runs and this space also gets used up quickly.In our experiment,we found that atleast 11GB of physical storage was needed,but if we switch 
   to T2.large instances then it would be more expensive.So,we chose to add more volume to T2.medium instance in the physical storage.This
   is feasible as EC2 instances have EBS storage and volume can be added on the fly.
   
   Configuring Elastic Ips for the instances:After allocating 4 instances,1 Namenode and 3 DataNodes of type T2.medium,go to the Network and
   Security section of the EC2 Mangaement console and allocate 4 new elastic IPs from the IPv4 address pool.After this IP is allocated,map this
   IP to any instance.Repeat this step for the remaining 3 instances.



